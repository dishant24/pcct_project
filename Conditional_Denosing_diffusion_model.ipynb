{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "LQnlc27k7Aiw",
    "outputId": "b8125880-d5b1-472a-d746-326c458e233e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF7wH4cE_JKY"
   },
   "source": [
    "# Building the Diffusion Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj17psVw7Shg"
   },
   "source": [
    "## Step 1: The forward process = Noise scheduler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWw50ui9IZ5q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps, start=0.0002, end=0.002):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "\n",
    "# Define beta schedule\n",
    "T = 800\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\"\n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Takes an image and a timestep as input and\n",
    "    returns the noisy version of it\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    )\n",
    "    # mean + variance\n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
    "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
    "\n",
    "def backward__diffusion_sample(x, t, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Calls the model to predict the noise in the image and returns \n",
    "        the denoised image. \n",
    "        Applies noise to this image, if we are not in the last step yet.\n",
    "        \"\"\"\n",
    "        betas_t = get_index_from_list(betas, t, x.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(torch.sqrt(1. - alphas_cumprod), t, x.shape)\n",
    "        sqrt_recip_alphas_t = get_index_from_list(torch.sqrt(1.0 / alphas), t, x.shape)\n",
    "        mean = sqrt_recip_alphas_t * (x - betas_t * model(x, t, **kwargs) / sqrt_one_minus_alphas_cumprod_t)\n",
    "        posterior_variance_t = betas_t\n",
    "\n",
    "        if t == 0:\n",
    "            return mean\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "            variance = torch.sqrt(posterior_variance_t) * noise \n",
    "            return mean + variance\n",
    "\n",
    "\n",
    "\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt6JSKawk7_b"
   },
   "source": [
    "Let's test it on our dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuckjpW_k1LN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def load_transformed_dataset():\n",
    "    data_transforms = [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(), # Scales data into [0,1]\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1]\n",
    "    ]\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "\n",
    "    train = torchvision.datasets.CIFAR10(root=\"data\", download=True,\n",
    "                                         transform=data_transform)\n",
    "    return train\n",
    "\n",
    "data = load_transformed_dataset()\n",
    "dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.cpu().numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    # Take first image of batch\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, :, :, :]\n",
    "    plt.imshow(reverse_transforms(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "id": "2fUPyJghdoUA",
    "outputId": "0a71ca76-dc54-481f-b845-0409e37d2f68",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simulate forward diffusion\n",
    "image = next(iter(dataloader))[0]\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axis('off')\n",
    "num_images = 10\n",
    "stepsize = int(T/num_images)\n",
    "\n",
    "for idx in range(0, T, stepsize):\n",
    "    t = torch.Tensor([idx]).type(torch.int64)\n",
    "    plt.subplot(1, num_images+1, int(idx/stepsize) + 1)\n",
    "    img, noise = forward_diffusion_sample(image, t)\n",
    "    show_tensor_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buW6BaNga-XH"
   },
   "source": [
    "## Step 2: The backward process = U-Net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOYPSxPf_LL7",
    "outputId": "54827b7e-b9c1-4ee5-d6da-07ec7d0e8af2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "        self.key = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "        self.value = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        proj_query = self.query(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key(x).view(batch_size, -1, width * height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        proj_value = self.value(x).view(batch_size, -1, width * height)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, channels, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, labels, up=False, with_attention=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        self.labels = labels\n",
    "        if labels:\n",
    "            self.label_mlp = nn.Linear(1, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.selu  = nn.SiLU()\n",
    "        self.with_attention = with_attention\n",
    "        if self.with_attention:\n",
    "            self.attention = SelfAttention(out_ch)\n",
    "\n",
    "    def forward(self, x, t, **kwargs):\n",
    "        # First Conv\n",
    "        h = self.relu(self.bnorm1(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.selu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        if self.labels:\n",
    "            label = kwargs.get('labels')\n",
    "            o_label = self.relu(self.label_mlp(label))\n",
    "            h = h + o_label[(..., ) + (None, ) * 2]\n",
    "        # Apply attention if required\n",
    "        if self.with_attention:\n",
    "            h = self.attention(h)\n",
    "        # Second Conv\n",
    "        h = self.relu(self.bnorm2(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SimpleUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified variant of the Unet architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, labels = False):\n",
    "        super().__init__()\n",
    "        image_channels = 3\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 3\n",
    "        time_emb_dim = 128\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1],\n",
    "                                          time_emb_dim, labels, with_attention=(i % 2 == 0))\n",
    "                                    for i in range(len(down_channels)-1)])\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1],\n",
    "                                        time_emb_dim, labels, up=True, with_attention=(i % 2 == 0))\n",
    "                                  for i in range(len(up_channels)-1)])\n",
    "\n",
    "        # Output convolution\n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
    "\n",
    "    def forward(self, x, timestep, **kwargs):\n",
    "        # Embedd time\n",
    "        t = self.time_mlp(timestep)\n",
    "        # Initial conv\n",
    "        x = self.conv0(x)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t, **kwargs)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            # Add residual x as additional channels\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t, **kwargs)\n",
    "        return self.output(x)\n",
    "\n",
    "model = SimpleUnet(labels=True)\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8B9GlZrotBXy"
   },
   "source": [
    "## Step 3: The loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7AZkYjKgQTm"
   },
   "source": [
    "## Sampling\n",
    "- Without adding @torch.no_grad() we quickly run out of memory, because pytorch tacks all the previous images for gradient calculation\n",
    "- Because we pre-calculated the noise variances for the forward pass, we also have to use them when we sequentially perform the backward process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "NUM_CLASSES = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k13hj2mciCHA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_timestep(x, t, c):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise in the image and returns\n",
    "    the denoised image.\n",
    "    Applies noise to this image, if we are not in the last step yet.\n",
    "    \"\"\"\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "\n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t, c) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "\n",
    "    if t == 0:\n",
    "        # As pointed out by Luis Pereira (see YouTube comment)\n",
    "        # The t's are offset from the t's in the paper\n",
    "        return model_mean\n",
    "    else:\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_plot_image(NUM_CLASSES):\n",
    "    # Sample noise\n",
    "    img_size = IMG_SIZE\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    num_images = 10\n",
    "    stepsize = int(T/num_images)\n",
    "    \n",
    "    for c in range(NUM_CLASSES):\n",
    "        imgs = torch.randn((1, 3) + (IMG_SIZE,IMG_SIZE)).to(device)\n",
    "        for i in range(0,T)[::-1]:\n",
    "            t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "            labels = torch.tensor([c] * 1).resize(1, 1).float().to(device)\n",
    "            img = sample_timestep(imgs, t, labels)\n",
    "            # Edit: This is to maintain the natural range of the distribution\n",
    "            img = torch.clamp(img, -1.0, 1.0)\n",
    "            if i % stepsize == 0:\n",
    "                plt.subplot(1, num_images, int(i/stepsize)+1)\n",
    "                show_tensor_image(img.detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIc33L9-uK4q"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bpN_LKYwuLx0",
    "outputId": "297f1c2c-4548-4e0a-84b6-7f6cd6234875",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0008)\n",
    "device = torch.device(\"cuda 1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 100 # Try more!\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(dataloader)\n",
    "    for step, (batch, label) in enumerate(pbar):\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).to(torch.int64)\n",
    "        x_noisy, noise = forward_diffusion_sample(batch, t, device)\n",
    "        predicted_noise = model(x_noisy, t, labels = label.reshape(-1,1).float().to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.nn.functional.mse_loss(noise, predicted_noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix(MSE=loss.item())\n",
    "\n",
    "        if epoch % 5 == 0 and step == 0:\n",
    "            print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "            # sample_plot_image(10)\n",
    "\n",
    "    if epoch % 10:\n",
    "        torch.save(model.state_dict(), f\"simple_diffusion_model{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = SimpleUnet(labels=True)\n",
    "unet.load_state_dict(torch.load(('simple_diffusion_model13')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (32, 32)\n",
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat')\n",
    "NUM_CLASSES = len(classes)\n",
    "NUM_DISPLAY_IMAGES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(16)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "f, ax = plt.subplots(NUM_CLASSES, NUM_DISPLAY_IMAGES, figsize = (100,100))\n",
    "\n",
    "for c in range(NUM_CLASSES):\n",
    "    imgs = torch.randn((NUM_DISPLAY_IMAGES, 3) + IMAGE_SHAPE).to(device)\n",
    "    for i in reversed(range(T)):\n",
    "        t = torch.full((1,), i, dtype=torch.long, device=device)\n",
    "        labels = torch.tensor([c] * NUM_DISPLAY_IMAGES).resize(NUM_DISPLAY_IMAGES, 1).float().to(device)\n",
    "        imgs = diffusion_model.backward(x=imgs, t=t, model=unet.eval().to(device), labels = labels)\n",
    "    for idx, img in enumerate(imgs):\n",
    "        ax[c][idx].imshow(reverse_transform(img))\n",
    "        ax[c][idx].set_title(f\"Class: {classes[c]}\", fontsize = 100)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(16)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "f, ax = plt.subplots(NUM_CLASSES, NUM_DISPLAY_IMAGES, figsize = (100,100))\n",
    "\n",
    "for c in range(NUM_CLASSES):\n",
    "    imgs = torch.randn((NUM_DISPLAY_IMAGES, 3) + IMAGE_SHAPE).to(device)\n",
    "    for i in reversed(range(T)):\n",
    "        t = torch.full((1,), i, dtype=torch.long, device=device)\n",
    "        labels = torch.tensor([c] * NUM_DISPLAY_IMAGES).resize(NUM_DISPLAY_IMAGES, 1).float().to(device)\n",
    "        imgs = backward__diffusion_sample(x=imgs, t=t, model=unet.eval().to(device), labels = labels)\n",
    "    for idx, img in enumerate(imgs):\n",
    "        ax[c][idx].imshow(reverse_transform(img))\n",
    "        ax[c][idx].set_title(f\"Class: {classes[c]}\", fontsize = 80)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wLHSIArLcFK0",
    "Rj17psVw7Shg",
    "buW6BaNga-XH",
    "8B9GlZrotBXy",
    "i7AZkYjKgQTm",
    "BIc33L9-uK4q"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
